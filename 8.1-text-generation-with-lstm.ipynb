{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with LSTM\n",
    "\n",
    "This notebook contains the code samples found in Chapter 8, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "[...]\n",
    "\n",
    "## Implementing character-level LSTM text generation\n",
    "\n",
    "\n",
    "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a \n",
    "language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this \n",
    "example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model \n",
    "we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the \n",
    "English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n",
      "preface\n",
      "\n",
      "\n",
      "supposing that truth is a woma\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# path = keras.utils.get_file(\n",
    "#     'nietzsche.txt',\n",
    "#     origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "# text = open(path).read().lower()\n",
    "# print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "# Use Reuters newswire data in place of the Nietzsche text\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = []\n",
    "\n",
    "# Get text from first 1000 news articles in training set\n",
    "for i in range(1000):\n",
    "    dn1 = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[i]])\n",
    "    dn2 = dn1.replace('?', '')\n",
    "    dn3 = dn2.replace('  ', ' ').lstrip()\n",
    "    decoded_newswire.append(dn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781278\n",
      "said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3 generale de banque sa lt br and lt heller overseas corp of chicago have each taken 50 pct stakes in company sa factors generale de banque said in a statement it gave no financial details of the transaction sa  turnover in 1986 was 17 5 billion belgian francs reuter 3 shr 3 28 dlrs vs 22 cts shr diluted 2 99 dlrs vs 22 cts net 46 0 mln vs 3 328 000 avg shrs 14 0 mln vs 15 2 mln year shr 5 41 dlrs vs 1 56 dlrs shr diluted 4 94 dlrs vs 1 50 dlrs net 78 2 mln vs 25 9 mln avg shrs 14 5 mln vs 15 1 mln note earnings per share reflect the two for one split effective january 6 1987 per share amounts are calculated af\n"
     ]
    }
   ],
   "source": [
    "# Join together to produce corpus of similar size to Nietzsche corpus (~800,000 characters)\n",
    "text = ' '.join(decoded_newswire)\n",
    "print(len(text))\n",
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will extract partially-overlapping sequences of length `maxlen`, one-hot encode them and pack them in a 3D Numpy array `x` of \n",
    "shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot \n",
    "encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 260406\n",
      "Unique characters: 38\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Our network is a single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. But let us note that \n",
    "recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in \n",
    "recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the language model and sampling from it\n",
    "\n",
    "\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, \n",
    "and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures \n",
    "after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of \n",
    "temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "260406/260406 [==============================] - 190s 731us/step - loss: 1.2577\n",
      "--- Generating with seed: \"11 3 4 cts vs 11 3 4 cts prior pay march 27 record march 13 \"\n",
      "------ temperature: 0.2\n",
      "11 3 4 cts vs 11 3 4 cts prior pay march 27 record march 13 shares of the company said the company said the company said the company said the becoming and the economy the offer the company said the company said the company said the business and the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company said the consimian said the company said the company said the bond of the company said the company said the company said the company said the company said the company said the company said the company sa\n",
      "------ temperature: 0.5\n",
      "ompany said the company said the company said the company said the consimiation commission to conditions to inc the company and the money be services and a problemsea where shares and the u s to continue to be commented and the desport and co of the indicated on the accounting to the foreign on said the distrins to the possible in the company and the discouncien state company and the new consider at the business control the commented the agreed to producti\n",
      "------ temperature: 1.0\n",
      "at the business control the commented the agreed to productions of diluse levels lt force 8 299 11 billion dlr railben the fund seas a be chemperian croles but for from co from said number and 1 945 billion dlrs shares in the consuger of had also beef the both asked all exchange so pper aleling managemist accord profit of september the excessd to the consumert at a 4 68 billion staff but of nowred and in strat on the economy negotiations in a hope to the b\n",
      "------ temperature: 1.2\n",
      " and in strat on the economy negotiations in a hope to the boliviams llany over group the lid we stipcc ibment under plan int oversey agency otering to geleing result and the u s states and said cooleurs and is level on spokesmans an exports wastering asbectively to posec es long tested shares betual one 100000 tounes a co and an adealibled a seagenen colombia a lawnon the fed of which bu ng amerinic no slrors december any caking indiveresal an february an\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "260406/260406 [==============================] - 165s 632us/step - loss: 1.2456\n",
      "--- Generating with seed: \"lization yeutter said yeutter did not mention the recent u s\"\n",
      "------ temperature: 0.2\n",
      "lization yeutter said yeutter did not mention the recent u s bank and the securities and the said the securities and a statement said the securities and the securities and the securities and company said the company and the securities and the company and the company said the company said the securities and a statement said the company said the securities and a market and company said the securities and said the securities and the said the statement and a p\n",
      "------ temperature: 0.5\n",
      "s and said the securities and the said the statement and a press and inc of the cross and a company and company bank was a new bowed the bank of participation to the amounts and the first 1989 once the destrent acquired that the company and the secutitive a dealers against the company said the company was increased by new remain that a shareholder that it had market the possible a port the restructuring in new trade shares and the proposed a statement to a\n",
      "------ temperature: 1.0\n",
      "turing in new trade shares and the proposed a statement to arab other a measurelad would bond about 129 4 criss  and january legisi analysts inflation business gain this yean said the reserves diss 1 and 1988 booss perpedy debt assets and state rishine ltd howeverral added the january nest rate of west goles seitions the useday business because and csaid report to reply rellenines bls in january a seculems acquiraly broge that but losses in this freed worl\n",
      "------ temperature: 1.2\n",
      " seculems acquiraly broge that but losses in this freed worldmand yould nt fraingnists collies lt dear prep paiding assets has s owners is cratific's cruding sea oper shr fr 3 385 000 dlrs vs prolive 38 irs holdina of 326 mln were four years about the 1987 uqutiows losseve ababanali  tuuss' mmerceal bringer provim tomorlolas serviower was after whold of 12 dlrs a cc stateaty sincial term streakay eart band business denciliaiary attake 49 met gatt co ltd al\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "260406/260406 [==============================] - 164s 632us/step - loss: 1.2351\n",
      "--- Generating with seed: \"ke no reserve management action in the u s government securi\"\n",
      "------ temperature: 0.2\n",
      "ke no reserve management action in the u s government securities and the company said the company said the company said it said the securities and the company said the company said the company said it also said the company said the company said the company said it said the be a stake in the company said the company said he said the state in the same and the company said the company said it also said it said the company said it also said it is and the produ\n",
      "------ temperature: 0.5\n",
      "id it said the company said it also said it is and the production of 1 1 pct of the results and and buy a yen inc said it said that federal year but a streage quarter substational and be and dimations the bundis but the partners he said it has a strengon said that the dealers said the restraind and as and acquires and the night petiod on the securities will be commercial bull prove the company said it is not one last year and and press the department said \n",
      "------ temperature: 1.0\n",
      "d it is not one last year and and press the department said rates and acheaper sevenad the sivies and sbick the dulitable indication for help shreleon but the bundates he said the korea the stark to the currencis ltd amportdy the implation the company is washingtontis from the begenical saver commission the stake more tennments co said in april 15 the company's meding gest for comprose aeghtld plantion has also were cruss under the decision said it has rep\n",
      "------ temperature: 1.2\n",
      "ntion has also were cruss under the decision said it has reported daisy niganilital ec stick during greath daive to 22 mln world high biy to gan options more he towed difficulty oil back one revenue growth o bunluadd and precemin bank 450 800 shortagoing political dolrs over exporter in january press change restoral chairman which suif coms departments clars ds were told sing 1o the thiggesid nomoss encevement on chemicals regur1 strainked 55 exports may q\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "260406/260406 [==============================] - 164s 632us/step - loss: 1.2266\n",
      "--- Generating with seed: \"1983 washington has been demanding a cut to seven pct equiva\"\n",
      "------ temperature: 0.2\n",
      "1983 washington has been demanding a cut to seven pct equivatel in the company said in a propose the tax company said the company said in a the company said the company said in the company said the state of the company said the company said the comment said in the company said the company said the proposal the company said the company said the company said the company said in the company said in the company said the first quarter and the company said the c\n",
      "------ temperature: 0.5\n",
      "he company said the first quarter and the company said the company said in the declined to the secroncy contract basis in the foreign and that a state reuter 3 the bill that the production to be bank and prices to be high billion market and comment proposal bank would continue to the proposal and the state for the fust of the company and crop share restated prices and state from 1986 that the report said in company said it has offer proceed to continue the\n",
      "------ temperature: 1.0\n",
      "rt said in company said it has offer proceed to continue the period the did not give told usay outlidels firms he added its we have assets coor offer bil dif teleben femiel purchase of state apply in the nends a meeting it irgesonaling volcces of fixed latest growers gre ronwelling their market or currency jocturing 1 600 mths increased step growth in attic cigy montal baribury from i farm to such ar1jut to while of supplies stell of valuational specialing\n",
      "------ temperature: 1.2\n",
      " ar1jut to while of supplies stell of valuational specialings achieved at minust insterond has extensions of member greecilities the wesk sperial hay of coind are minmant furthio no baaching taiwhhan terateving friden and but becomi tender 32 9 pct to be between market roteb prices aboct last u s rising and unloting the credit of batevent sencesy vacturterality world 37 07 billion belli6sly to whole crilisiin brithstary sapraces and shares is two until pas\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in \n",
    "particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text \n",
    "becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \n",
    "\"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings \n",
    "of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment \n",
    "with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and \n",
    "realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is \n",
    "sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is \n",
    "a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To \n",
    "evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like \n",
    "our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic \n",
    "statistical structure, thus making it impossible to learn a language model like we just did.\n",
    "\n",
    "\n",
    "## Take aways\n",
    "\n",
    "* We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
    "* In the case of text, such a model is called a \"language model\" and could be based on either words or characters.\n",
    "* Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
    "* One way to handle this is the notion of _softmax temperature_. Always experiment with different temperatures to find the \"right\" one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
